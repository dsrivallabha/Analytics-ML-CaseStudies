{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6b8bba-bcc2-4520-b283-68061e2cdd94",
   "metadata": {},
   "source": [
    "# Tic Tac Toe\n",
    "With Neural Network for State Value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b125a8c-23df-4ade-b33e-067a24fdd36b",
   "metadata": {},
   "source": [
    "### Outline of approach:\n",
    "1. Pretrain: Play atleast 100 games and get the training data for state and values as list.  \\\n",
    "    Train a 2 value predictor network on this data - one for P1 and another for P2\n",
    "    In the first step, predictor network gives random values, that are used for the first batch of games\n",
    "2. RL Train: In a loop \\\n",
    "    a) play n games using the trained networks and epsilon greedy approach \\\n",
    "    b) record the outcomes and compute state values \\\n",
    "    c) use this data to retrain the two networks \\\n",
    "3. Train till convergence\n",
    "\n",
    "Reference url for Tic Tac Toe environment: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/TicTacToe/ticTacToe.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b46ea6-803e-4b9f-97cf-ac20b66a71b8",
   "metadata": {},
   "source": [
    "### Basic package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "271416f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 17:40:13.807398: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-28 17:40:13.807430: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69245fc0-915e-40db-a1fc-945136814a7b",
   "metadata": {},
   "source": [
    "### Classes and Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613774a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    '''\n",
    "    Definition of a Tic-Tac-Toe board\n",
    "    '''\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "\n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "\n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(-1)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(-1)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(0)\n",
    "\n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    def play(self, rounds=100):\n",
    "        winlist = []\n",
    "        for i in range(rounds):\n",
    "            #if i % 1000 == 0:\n",
    "            #    print(\"Rounds {}\".format(i))\n",
    "            if i % 100 == 0:\n",
    "                self.p1.setEps(rounds, i)\n",
    "                self.p2.setEps(rounds, i)\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "\n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "            winlist.append(win)\n",
    "        return (winlist)\n",
    "    # play with human\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "    def NNPlay(self, rounds=500, innerrounds=50):\n",
    "        '''\n",
    "        train innerrounds, capture replay buffer and train the s value networks on this data\n",
    "        after every inner round the replay buffer is emptied and the process repeated again\n",
    "        '''\n",
    "        train_batches = int(rounds/innerrounds) + 1\n",
    "        \n",
    "        print ('training for {} rounds, with {} training batches and {} inner rounds'.format(rounds, train_batches, innerrounds))\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "            print ('training batch', i)\n",
    "            self.play(innerrounds)\n",
    "            # train player p1\n",
    "            self.p1.sVNNtrain()\n",
    "            # train player p2\n",
    "            self.p2.sVNNtrain()\n",
    "        return ()\n",
    "    \n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d1328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Class for one Tic Tac Toe player\n",
    "    '''\n",
    "    def __init__(self, name, eps_decay=False, start_exp_rate=0.3, end_exp_rate=0.05):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.3\n",
    "        self.exp_rate = start_exp_rate\n",
    "        \n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "\n",
    "        self.eps_decay = eps_decay\n",
    "        self.start_exp_rate = start_exp_rate\n",
    "        self.end_exp_rate = end_exp_rate\n",
    "        self.state_value_model = self.sValueNN()\n",
    "        \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return boardHash\n",
    "    \n",
    "    def getSVal(self, board):\n",
    "        bs = board.reshape(BOARD_COLS * BOARD_ROWS)\n",
    "        bost = np.reshape(bs, (-1, 9))\n",
    "        sval = self.state_value_model.predict(bost, verbose=False)[0][0]\n",
    "        return sval\n",
    "    \n",
    "    def sValueNN(self):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(Dense(units=4, input_dim=9, activation='linear'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "        return (model)\n",
    "    \n",
    "    def getbuffer(self):\n",
    "        data = self.states_value\n",
    "        ll = []\n",
    "        for k in data.keys():\n",
    "            yy = re.findall( r'[-/+]?\\d+\\.*\\d*', k)\n",
    "            zz = data.get(k)\n",
    "            yy.append(zz)\n",
    "            ll.append(yy)\n",
    "        lldf = pd.DataFrame(ll)\n",
    "        cols = ['x'+str(i) for i in range(9)]\n",
    "        cols.append('val')\n",
    "        lldf.columns = cols\n",
    "        lldf.to_csv('Buffer.csv', index=False)\n",
    "        return \n",
    "\n",
    "    def sVNNtrain(self, Xin = None, Yin = None):\n",
    "        # call this function to create buffer\n",
    "        self.getbuffer()\n",
    "        # read the buffer\n",
    "        df = pd.read_csv('Buffer.csv')\n",
    "        print ('length of buffer is', len(df))\n",
    "        traincols=['x' + str(i) for i in range(9)]\n",
    "        testcol = 'val'\n",
    "        if Xin is None:    \n",
    "            Xin = df[traincols]\n",
    "            Yin = df[testcol]\n",
    "        self.state_value_model.fit(Xin, Yin, epochs=10, verbose=False)\n",
    "        \n",
    "        # empty the replay buffer after this \n",
    "        self.states_value = {}\n",
    "        return \n",
    "\n",
    "    def loadSVNNmodel(self, path):\n",
    "        self.state_value_model = keras.models.load_model(path)\n",
    "    \n",
    "    def setEps(self, total_games, current_game):\n",
    "        if (self.eps_decay == False):\n",
    "            return\n",
    "        else:\n",
    "            if int(current_game/100) > 0:   \n",
    "            #if (np.mod(current_game+1, 100) == 0):\n",
    "                self.exp_rate = self.start_exp_rate * (1. - current_game/total_games) + self.end_exp_rate * (current_game/total_games)\n",
    "                if (np.mod(current_game, 1000) == 0):\n",
    "                    print ('decay rate modified at {} with current value of {}'.format(str(current_game), str(self.exp_rate)))\n",
    "        return \n",
    "\n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # Default method uses a dictionary to get the value of a state\n",
    "                #value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                \n",
    "                # New method use the keras model to predict the state value \n",
    "                value = self.getSVal(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0645a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    '''\n",
    "    Class for a human player\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "    ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab371d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 17:40:15.239590: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-01-28 17:40:15.239612: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-01-28 17:40:15.239629: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dsrivallabha-PC): /proc/driver/nvidia/version does not exist\n",
      "2024-01-28 17:40:15.240076: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"p1\", eps_decay = False, start_exp_rate=0.2, end_exp_rate=0.02)\n",
    "p2 = Player(\"p2\", eps_decay = False, start_exp_rate=0.2, end_exp_rate=0.02)\n",
    "print (p1.exp_rate, p2.exp_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf520f9a-879a-4697-8557-3c772f812e02",
   "metadata": {},
   "source": [
    "# Examine the defined model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edba14b3-1ad4-4896-9fc2-3c9cd69adc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 40        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45\n",
      "Trainable params: 45\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "p1.state_value_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc875407-20de-4197-b421-bf5fdc38a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.24858743, -0.21132362, -0.37820578,  0.09079397],\n",
      "       [-0.6750407 ,  0.00411654, -0.6061317 ,  0.29925823],\n",
      "       [ 0.33388782, -0.5875468 ,  0.0451318 ,  0.16753101],\n",
      "       [ 0.4180441 ,  0.58865845,  0.05096269,  0.5291933 ],\n",
      "       [-0.01286185,  0.5965493 , -0.24636659,  0.26979625],\n",
      "       [-0.19535029,  0.17613   ,  0.48654425, -0.48584723],\n",
      "       [ 0.30961984, -0.3161221 , -0.6518061 , -0.14878696],\n",
      "       [ 0.24420196, -0.28799623, -0.31704247,  0.67030394],\n",
      "       [-0.07619268, -0.63899237, -0.01540369, -0.16941732]],\n",
      "      dtype=float32), array([0., 0., 0., 0.], dtype=float32)]\n",
      "{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[0.6282011 ],\n",
      "       [0.02111208],\n",
      "       [0.23713493],\n",
      "       [0.63207483]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = p1.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83094fcc-15f1-4583-aaf1-901dc7e47fb3",
   "metadata": {},
   "source": [
    "## Train the two players, with constant epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcc895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 2000 rounds, with 41 training batches and 50 inner rounds\n",
      "training batch 0\n",
      "length of buffer is 72\n",
      "length of buffer is 67\n",
      "training batch 1\n",
      "length of buffer is 68\n",
      "length of buffer is 69\n",
      "training batch 2\n",
      "length of buffer is 57\n",
      "length of buffer is 49\n",
      "training batch 3\n",
      "length of buffer is 61\n",
      "length of buffer is 52\n",
      "training batch 4\n",
      "length of buffer is 51\n",
      "length of buffer is 51\n",
      "training batch 5\n",
      "length of buffer is 52\n",
      "length of buffer is 47\n",
      "training batch 6\n",
      "length of buffer is 72\n",
      "length of buffer is 64\n",
      "training batch 7\n",
      "length of buffer is 53\n",
      "length of buffer is 47\n",
      "training batch 8\n",
      "length of buffer is 59\n",
      "length of buffer is 57\n",
      "training batch 9\n",
      "length of buffer is 54\n",
      "length of buffer is 58\n",
      "training batch 10\n",
      "length of buffer is 65\n",
      "length of buffer is 57\n",
      "training batch 11\n",
      "length of buffer is 50\n",
      "length of buffer is 48\n",
      "training batch 12\n",
      "length of buffer is 61\n",
      "length of buffer is 58\n",
      "training batch 13\n",
      "length of buffer is 65\n",
      "length of buffer is 60\n",
      "training batch 14\n",
      "length of buffer is 64\n",
      "length of buffer is 55\n",
      "training batch 15\n",
      "length of buffer is 68\n",
      "length of buffer is 68\n",
      "training batch 16\n",
      "length of buffer is 53\n",
      "length of buffer is 48\n",
      "training batch 17\n",
      "length of buffer is 69\n",
      "length of buffer is 69\n",
      "training batch 18\n",
      "length of buffer is 77\n",
      "length of buffer is 71\n",
      "training batch 19\n",
      "length of buffer is 64\n",
      "length of buffer is 58\n",
      "training batch 20\n",
      "length of buffer is 64\n",
      "length of buffer is 56\n",
      "training batch 21\n",
      "length of buffer is 58\n",
      "length of buffer is 54\n",
      "training batch 22\n",
      "length of buffer is 58\n",
      "length of buffer is 51\n",
      "training batch 23\n",
      "length of buffer is 64\n",
      "length of buffer is 62\n",
      "training batch 24\n",
      "length of buffer is 54\n",
      "length of buffer is 52\n",
      "training batch 25\n",
      "length of buffer is 48\n",
      "length of buffer is 47\n",
      "training batch 26\n",
      "length of buffer is 53\n",
      "length of buffer is 47\n",
      "training batch 27\n",
      "length of buffer is 64\n",
      "length of buffer is 60\n",
      "training batch 28\n",
      "length of buffer is 65\n",
      "length of buffer is 62\n",
      "training batch 29\n",
      "length of buffer is 64\n",
      "length of buffer is 58\n",
      "training batch 30\n",
      "length of buffer is 59\n",
      "length of buffer is 52\n",
      "training batch 31\n",
      "length of buffer is 57\n",
      "length of buffer is 51\n",
      "training batch 32\n",
      "length of buffer is 74\n",
      "length of buffer is 64\n",
      "training batch 33\n",
      "length of buffer is 62\n",
      "length of buffer is 57\n",
      "training batch 34\n",
      "length of buffer is 59\n",
      "length of buffer is 58\n",
      "training batch 35\n",
      "length of buffer is 71\n",
      "length of buffer is 71\n",
      "training batch 36\n",
      "length of buffer is 54\n",
      "length of buffer is 47\n",
      "training batch 37\n",
      "length of buffer is 67\n",
      "length of buffer is 59\n",
      "training batch 38\n",
      "length of buffer is 55\n",
      "length of buffer is 50\n",
      "training batch 39\n",
      "length of buffer is 57\n",
      "length of buffer is 58\n",
      "training batch 40\n",
      "length of buffer is 64\n",
      "length of buffer is 62\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "st = State(p1, p2)\n",
    "st.NNPlay(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9300b7c4-f226-4837-8e63-1ab962531e6a",
   "metadata": {},
   "source": [
    "### Examine and save the keras model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9aeacc-8018-411c-a566-548bd0a64390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.40683657, -0.4158106 , -0.32265148, -0.30908552],\n",
      "       [-0.3512696 ,  0.29168206,  0.4053993 ,  0.3450481 ],\n",
      "       [ 0.15070567, -0.25628412, -0.00177603, -0.08401844],\n",
      "       [-0.33369794, -0.55786455, -0.13194144,  0.03971705],\n",
      "       [ 0.2153035 , -0.23054773, -0.6323803 , -0.5408313 ],\n",
      "       [ 0.3124842 , -0.56576216,  0.2588895 ,  0.21737023],\n",
      "       [-0.24144727,  0.14803237, -0.4194094 , -0.54073966],\n",
      "       [ 0.36176884, -0.3991822 , -0.13209403, -0.12980594],\n",
      "       [-0.16940153,  0.15293834,  0.32446122,  0.0399254 ]],\n",
      "      dtype=float32), array([-0.12354146,  0.05740879, -0.0255044 ,  0.08322165], dtype=float32)]\n",
      "{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.00316045],\n",
      "       [-0.08580635],\n",
      "       [ 0.39758286],\n",
      "       [-0.4886443 ]], dtype=float32), array([0.0072863], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = st.p1.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa17b86-c74e-4881-9597-afdd1bfe0c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense_2', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.02253479, -0.33591506,  0.31116265, -0.4227688 ],\n",
      "       [-0.03243079,  0.34823313, -0.1830587 , -0.64018464],\n",
      "       [ 0.5267765 , -0.26305583, -0.25569472,  0.19048546],\n",
      "       [-0.42434773,  0.06299321,  0.12105923,  0.00781979],\n",
      "       [ 0.1615687 , -0.43870527, -0.00705629,  0.01748821],\n",
      "       [-0.6103813 ,  0.32546517,  0.01076336,  0.47630298],\n",
      "       [-0.26896507, -0.12461756,  0.38393894, -0.51620823],\n",
      "       [-0.5953689 , -0.17032419,  0.41177627, -0.3451145 ],\n",
      "       [ 0.04939187, -0.34967712,  0.1777797 , -0.3504383 ]],\n",
      "      dtype=float32), array([0.02196499, 0.0293889 , 0.02589977, 0.00233336], dtype=float32)]\n",
      "{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.3657348 ],\n",
      "       [-0.38401392],\n",
      "       [-0.6381139 ],\n",
      "       [-0.12606573]], dtype=float32), array([-0.03456396], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = st.p2.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f5e622-c8cc-49e0-8a63-ecb5ddd72466",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.p1.state_value_model.save('p1model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5fdf71-a8d4-40ce-be80-04e027009444",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.p2.state_value_model.save('p2model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dafc17-a8d7-4ba1-988d-2d0be111ed1a",
   "metadata": {},
   "source": [
    "# Check learning by playing against opponent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea873566-dd64-488e-b8b4-72a6e6d0171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained first player as p3\n",
    "p3 = Player(\"p3\", eps_decay = False, start_exp_rate=0, end_exp_rate=0)\n",
    "p3.loadSVNNmodel('p1model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4197722-cdb0-4406-b050-af77fab00153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second player is a random player. Setting start_exp_rate = 1 makes all actions by this player as random\n",
    "p4 = Player(\"p4\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c145d7-c029-4da2-8eae-0f2d5e178cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expt 0 in this batch of 100 games trained p1 wins 76, random p2 wins 13 and ties 11\n",
      "expt 1 in this batch of 100 games trained p1 wins 75, random p2 wins 14 and ties 11\n",
      "expt 2 in this batch of 100 games trained p1 wins 72, random p2 wins 20 and ties 8\n",
      "expt 3 in this batch of 100 games trained p1 wins 74, random p2 wins 19 and ties 7\n",
      "expt 4 in this batch of 100 games trained p1 wins 77, random p2 wins 12 and ties 11\n",
      "expt 5 in this batch of 100 games trained p1 wins 74, random p2 wins 20 and ties 6\n",
      "expt 6 in this batch of 100 games trained p1 wins 80, random p2 wins 10 and ties 10\n",
      "expt 7 in this batch of 100 games trained p1 wins 77, random p2 wins 10 and ties 13\n",
      "expt 8 in this batch of 100 games trained p1 wins 77, random p2 wins 12 and ties 11\n",
      "expt 9 in this batch of 100 games trained p1 wins 79, random p2 wins 12 and ties 9\n"
     ]
    }
   ],
   "source": [
    "# Play multiple batches of 100 games with trained P3 and random P4 and evaluate the winning probabilties\n",
    "st = State(p3, p4)\n",
    "\n",
    "for i in range(10):\n",
    "    output = st.play(100)\n",
    "    print ('expt {} in this batch of 100 games trained p1 wins {}, random p2 wins {} and ties {}'.format(i, output.count(1), output.count(-1), output.count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84be7c-43fe-4267-96f5-ba6e12bd3833",
   "metadata": {},
   "source": [
    "### Compare against base line, when two random players play against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e987afa-fb83-40db-a715-72db0eb59169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learning by playing two random players against each other\n",
    "p5 = Player(\"p5\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)\n",
    "p6 = Player(\"p6\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ae1e413-51fb-44e1-9cac-6c5623617920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expt 0 in this batch of 100 games random p1 wins 54, random p2 wins 30 and ties 16\n",
      "expt 1 in this batch of 100 games random p1 wins 61, random p2 wins 23 and ties 16\n",
      "expt 2 in this batch of 100 games random p1 wins 62, random p2 wins 24 and ties 14\n",
      "expt 3 in this batch of 100 games random p1 wins 62, random p2 wins 24 and ties 14\n",
      "expt 4 in this batch of 100 games random p1 wins 61, random p2 wins 26 and ties 13\n",
      "expt 5 in this batch of 100 games random p1 wins 62, random p2 wins 24 and ties 14\n",
      "expt 6 in this batch of 100 games random p1 wins 59, random p2 wins 27 and ties 14\n",
      "expt 7 in this batch of 100 games random p1 wins 59, random p2 wins 30 and ties 11\n",
      "expt 8 in this batch of 100 games random p1 wins 67, random p2 wins 24 and ties 9\n",
      "expt 9 in this batch of 100 games random p1 wins 57, random p2 wins 31 and ties 12\n"
     ]
    }
   ],
   "source": [
    "st = State(p5, p6)\n",
    "\n",
    "for i in range(10):\n",
    "    output = st.play(100)\n",
    "    print ('expt {} in this batch of 100 games random p1 wins {}, random p2 wins {} and ties {}'.format(i, output.count(1), output.count(-1), output.count(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f145a5-1178-4a8b-b68e-3b9f8c018a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
