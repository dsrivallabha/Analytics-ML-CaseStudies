{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6b8bba-bcc2-4520-b283-68061e2cdd94",
   "metadata": {},
   "source": [
    "# Tic Tac Toe\n",
    "With Neural Network for State Value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b125a8c-23df-4ade-b33e-067a24fdd36b",
   "metadata": {},
   "source": [
    "### Outline of approach:\n",
    "1. Pretrain: Play atleast 100 games and get the training data for state and values as list.  \\\n",
    "    Train a 2 value predictor network on this data - one for P1 and another for P2  \n",
    "2. RL Train: In a loop \\\n",
    "    a) play n games using the trained networks and epsilon greedy approach \\\n",
    "    b) record the outcomes and compute state values \\\n",
    "    c) use this data to retrain the two networks \\\n",
    "3. Train till convergence\n",
    "\n",
    "Reference url for Tic Tac Toe environment: https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/TicTacToe/ticTacToe.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b46ea6-803e-4b9f-97cf-ac20b66a71b8",
   "metadata": {},
   "source": [
    "### Basic package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271416f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 13:45:58.301394: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-28 13:45:58.301411: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Activation\n",
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69245fc0-915e-40db-a1fc-945136814a7b",
   "metadata": {},
   "source": [
    "### Classes and Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613774a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    '''\n",
    "    Definition of a Tic-Tac-Toe board\n",
    "    '''\n",
    "    def __init__(self, p1, p2):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.isEnd = False\n",
    "        self.boardHash = None\n",
    "        # init p1 plays first\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    # get unique hash of current board state\n",
    "    def getHash(self):\n",
    "        self.boardHash = str(self.board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return self.boardHash\n",
    "\n",
    "    def winner(self):\n",
    "        # row\n",
    "        for i in range(BOARD_ROWS):\n",
    "            if sum(self.board[i, :]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[i, :]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # col\n",
    "        for i in range(BOARD_COLS):\n",
    "            if sum(self.board[:, i]) == 3:\n",
    "                self.isEnd = True\n",
    "                return 1\n",
    "            if sum(self.board[:, i]) == -3:\n",
    "                self.isEnd = True\n",
    "                return -1\n",
    "        # diagonal\n",
    "        diag_sum1 = sum([self.board[i, i] for i in range(BOARD_COLS)])\n",
    "        diag_sum2 = sum([self.board[i, BOARD_COLS - i - 1] for i in range(BOARD_COLS)])\n",
    "        diag_sum = max(abs(diag_sum1), abs(diag_sum2))\n",
    "        if diag_sum == 3:\n",
    "            self.isEnd = True\n",
    "            if diag_sum1 == 3 or diag_sum2 == 3:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "\n",
    "        # tie\n",
    "        # no available positions\n",
    "        if len(self.availablePositions()) == 0:\n",
    "            self.isEnd = True\n",
    "            return 0\n",
    "        # not end\n",
    "        self.isEnd = False\n",
    "        return None\n",
    "\n",
    "    def availablePositions(self):\n",
    "        positions = []\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    positions.append((i, j))  # need to be tuple\n",
    "        return positions\n",
    "\n",
    "    def updateState(self, position):\n",
    "        self.board[position] = self.playerSymbol\n",
    "        # switch to another player\n",
    "        self.playerSymbol = -1 if self.playerSymbol == 1 else 1\n",
    "\n",
    "    # only when game ends\n",
    "    def giveReward(self):\n",
    "        result = self.winner()\n",
    "        # backpropagate reward\n",
    "        if result == 1:\n",
    "            self.p1.feedReward(1)\n",
    "            self.p2.feedReward(-1)\n",
    "        elif result == -1:\n",
    "            self.p1.feedReward(-1)\n",
    "            self.p2.feedReward(1)\n",
    "        else:\n",
    "            self.p1.feedReward(0)\n",
    "            self.p2.feedReward(0)\n",
    "\n",
    "    # board reset\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
    "        self.boardHash = None\n",
    "        self.isEnd = False\n",
    "        self.playerSymbol = 1\n",
    "\n",
    "    def play(self, rounds=100):\n",
    "        winlist = []\n",
    "        for i in range(rounds):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Rounds {}\".format(i))\n",
    "            if i % 100 == 0:\n",
    "                self.p1.setEps(rounds, i)\n",
    "                self.p2.setEps(rounds, i)\n",
    "            while not self.isEnd:\n",
    "                # Player 1\n",
    "                positions = self.availablePositions()\n",
    "                p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                # take action and upate board state\n",
    "                self.updateState(p1_action)\n",
    "                board_hash = self.getHash()\n",
    "                self.p1.addState(board_hash)\n",
    "                # check board status if it is end\n",
    "\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    # self.showBoard()\n",
    "                    # ended with p1 either win or draw\n",
    "                    self.giveReward()\n",
    "                    self.p1.reset()\n",
    "                    self.p2.reset()\n",
    "                    self.reset()\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # Player 2\n",
    "                    positions = self.availablePositions()\n",
    "                    p2_action = self.p2.chooseAction(positions, self.board, self.playerSymbol)\n",
    "                    self.updateState(p2_action)\n",
    "                    board_hash = self.getHash()\n",
    "                    self.p2.addState(board_hash)\n",
    "\n",
    "                    win = self.winner()\n",
    "                    if win is not None:\n",
    "                        # self.showBoard()\n",
    "                        # ended with p2 either win or draw\n",
    "                        self.giveReward()\n",
    "                        self.p1.reset()\n",
    "                        self.p2.reset()\n",
    "                        self.reset()\n",
    "                        break\n",
    "            winlist.append(win)\n",
    "        return (winlist)\n",
    "    # play with human\n",
    "    def play2(self):\n",
    "        while not self.isEnd:\n",
    "            # Player 1\n",
    "            positions = self.availablePositions()\n",
    "            p1_action = self.p1.chooseAction(positions, self.board, self.playerSymbol)\n",
    "            # take action and upate board state\n",
    "            self.updateState(p1_action)\n",
    "            self.showBoard()\n",
    "            # check board status if it is end\n",
    "            win = self.winner()\n",
    "            if win is not None:\n",
    "                if win == 1:\n",
    "                    print(self.p1.name, \"wins!\")\n",
    "                else:\n",
    "                    print(\"tie!\")\n",
    "                self.reset()\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Player 2\n",
    "                positions = self.availablePositions()\n",
    "                p2_action = self.p2.chooseAction(positions)\n",
    "\n",
    "                self.updateState(p2_action)\n",
    "                self.showBoard()\n",
    "                win = self.winner()\n",
    "                if win is not None:\n",
    "                    if win == -1:\n",
    "                        print(self.p2.name, \"wins!\")\n",
    "                    else:\n",
    "                        print(\"tie!\")\n",
    "                    self.reset()\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "    def NNPlay(self, rounds=500, innerrounds=50):\n",
    "        '''\n",
    "        train innerrounds, capture replay buffer and train the s value networks on this data\n",
    "        after every inner round the replay buffer is emptied and the process repeated again\n",
    "        '''\n",
    "        train_batches = int(rounds/innerrounds) + 1\n",
    "        \n",
    "        print ('training for {} rounds, with {} training batches and {} inner rounds'.format(rounds, train_batches, innerrounds))\n",
    "        \n",
    "        for i in range(train_batches):\n",
    "            print ('training batch', i)\n",
    "            self.play(innerrounds)\n",
    "            # train player p1\n",
    "            self.p1.sVNNtrain()\n",
    "            # train player p2\n",
    "            self.p2.sVNNtrain()\n",
    "        return ()\n",
    "    \n",
    "    def showBoard(self):\n",
    "        # p1: x  p2: o\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = 'x'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'o'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = ' '\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d1328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    '''\n",
    "    Class for one Tic Tac Toe player\n",
    "    '''\n",
    "    def __init__(self, name, eps_decay=False, start_exp_rate=0.3, end_exp_rate=0.05):\n",
    "        self.name = name\n",
    "        self.states = []  # record all positions taken\n",
    "        self.lr = 0.3\n",
    "        self.exp_rate = start_exp_rate\n",
    "        \n",
    "        self.decay_gamma = 0.9\n",
    "        self.states_value = {}  # state -> value\n",
    "\n",
    "        self.eps_decay = eps_decay\n",
    "        self.start_exp_rate = start_exp_rate\n",
    "        self.end_exp_rate = end_exp_rate\n",
    "        self.state_value_model = self.sValueNN()\n",
    "        \n",
    "    def getHash(self, board):\n",
    "        boardHash = str(board.reshape(BOARD_COLS * BOARD_ROWS))\n",
    "        return boardHash\n",
    "    \n",
    "    def getSVal(self, board):\n",
    "        bs = board.reshape(BOARD_COLS * BOARD_ROWS)\n",
    "        bost = np.reshape(bs, (-1, 9))\n",
    "        sval = self.state_value_model.predict(bost, verbose=False)[0][0]\n",
    "        return sval\n",
    "    \n",
    "    def sValueNN(self):\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(Dense(units=4, input_dim=9, activation='linear'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "        return (model)\n",
    "    \n",
    "    def getbuffer(self):\n",
    "        data = self.states_value\n",
    "        ll = []\n",
    "        for k in data.keys():\n",
    "            yy = re.findall( r'[-/+]?\\d+\\.*\\d*', k)\n",
    "            zz = data.get(k)\n",
    "            yy.append(zz)\n",
    "            ll.append(yy)\n",
    "        lldf = pd.DataFrame(ll)\n",
    "        cols = ['x'+str(i) for i in range(9)]\n",
    "        cols.append('val')\n",
    "        lldf.columns = cols\n",
    "        lldf.to_csv('Buffer.csv', index=False)\n",
    "        return \n",
    "\n",
    "    def sVNNtrain(self, Xin = None, Yin = None):\n",
    "        # call this function to create buffer\n",
    "        self.getbuffer()\n",
    "        # read the buffer\n",
    "        df = pd.read_csv('Buffer.csv')\n",
    "        print ('length of buffer is', len(df))\n",
    "        traincols=['x' + str(i) for i in range(9)]\n",
    "        testcol = 'val'\n",
    "        if Xin is None:    \n",
    "            Xin = df[traincols]\n",
    "            Yin = df[testcol]\n",
    "        self.state_value_model.fit(Xin, Yin, epochs=10, verbose=False)\n",
    "        \n",
    "        # empty the replay buffer after this \n",
    "        self.states_value = {}\n",
    "        return \n",
    "\n",
    "    def loadSVNNmodel(self, path):\n",
    "        self.state_value_model = keras.models.load_model(path)\n",
    "    \n",
    "    def setEps(self, total_games, current_game):\n",
    "        if (self.eps_decay == False):\n",
    "            return\n",
    "        else:\n",
    "            if int(current_game/100) > 0:   \n",
    "            #if (np.mod(current_game+1, 100) == 0):\n",
    "                self.exp_rate = self.start_exp_rate * (1. - current_game/total_games) + self.end_exp_rate * (current_game/total_games)\n",
    "                if (np.mod(current_game, 1000) == 0):\n",
    "                    print ('decay rate modified at {} with current value of {}'.format(str(current_game), str(self.exp_rate)))\n",
    "        return \n",
    "\n",
    "    def chooseAction(self, positions, current_board, symbol):\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # take random action\n",
    "            idx = np.random.choice(len(positions))\n",
    "            action = positions[idx]\n",
    "        else:\n",
    "            value_max = -999\n",
    "            for p in positions:\n",
    "                next_board = current_board.copy()\n",
    "                next_board[p] = symbol\n",
    "                next_boardHash = self.getHash(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # Default method uses a dictionary to get the value of a state\n",
    "                #value = 0 if self.states_value.get(next_boardHash) is None else self.states_value.get(next_boardHash)\n",
    "                \n",
    "                # New method use the keras model to predict the state value \n",
    "                value = self.getSVal(next_board)\n",
    "                #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                # print(\"value\", value)\n",
    "                if value >= value_max:\n",
    "                    value_max = value\n",
    "                    action = p\n",
    "        # print(\"{} takes action {}\".format(self.name, action))\n",
    "        return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        self.states.append(state)\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_value.get(st) is None:\n",
    "                self.states_value[st] = 0\n",
    "            self.states_value[st] += self.lr * (self.decay_gamma * reward - self.states_value[st])\n",
    "            reward = self.states_value[st]\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "\n",
    "    def savePolicy(self):\n",
    "        fw = open('policy_' + str(self.name), 'wb')\n",
    "        pickle.dump(self.states_value, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def loadPolicy(self, file):\n",
    "        fr = open(file, 'rb')\n",
    "        self.states_value = pickle.load(fr)\n",
    "        fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0645a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer:\n",
    "    '''\n",
    "    Class for a human player\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def chooseAction(self, positions):\n",
    "        while True:\n",
    "            row = int(input(\"Input your action row:\"))\n",
    "            col = int(input(\"Input your action col:\"))\n",
    "            action = (row, col)\n",
    "            if action in positions:\n",
    "                return action\n",
    "\n",
    "    # append a hash state\n",
    "    def addState(self, state):\n",
    "        pass\n",
    "\n",
    "    # at the end of game, backpropagate and update states value\n",
    "    def feedReward(self, reward):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "    ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6807b50-30a4-4c93-876c-e03510f69121",
   "metadata": {},
   "source": [
    "def definemodel():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Dense(units=4, input_dim=9, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    return (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00d8e9-3f5e-4237-9529-84126d5a4fc6",
   "metadata": {},
   "source": [
    "Define the NN models for P1 and P2, one time. \\\n",
    "They do not change during the course of training, only the weights get updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab371d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 13:45:59.299386: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-01-28 13:45:59.299411: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-01-28 13:45:59.299427: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dsrivallabha-PC): /proc/driver/nvidia/version does not exist\n",
      "2024-01-28 13:45:59.299881: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "p1 = Player(\"p1\", eps_decay = False, start_exp_rate=0.2, end_exp_rate=0.02)\n",
    "p2 = Player(\"p2\", eps_decay = False, start_exp_rate=0.2, end_exp_rate=0.02)\n",
    "#p2.setEps(5000, 209)\n",
    "print (p1.exp_rate, p2.exp_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf520f9a-879a-4697-8557-3c772f812e02",
   "metadata": {},
   "source": [
    "# Examine the defined model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edba14b3-1ad4-4896-9fc2-3c9cd69adc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 40        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45\n",
      "Trainable params: 45\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "p1.state_value_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc875407-20de-4197-b421-bf5fdc38a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.24497217, -0.43987298, -0.27615482, -0.03567499],\n",
      "       [ 0.6102681 ,  0.15904182, -0.16795164,  0.3927878 ],\n",
      "       [ 0.18159759,  0.46008253, -0.59124386, -0.11502939],\n",
      "       [-0.25175968,  0.40120637,  0.60146165,  0.24036628],\n",
      "       [ 0.17378628,  0.64320064, -0.64446485,  0.6148833 ],\n",
      "       [-0.17706686, -0.11982399, -0.02513134, -0.4752825 ],\n",
      "       [-0.26079535, -0.6009986 ,  0.57542455, -0.56894886],\n",
      "       [ 0.1557194 ,  0.43464303, -0.33834165, -0.4720738 ],\n",
      "       [-0.26055256,  0.1727522 , -0.4327246 ,  0.39834523]],\n",
      "      dtype=float32), array([0., 0., 0., 0.], dtype=float32)]\n",
      "{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 1.0005672 ],\n",
      "       [-0.03970921],\n",
      "       [-0.16657132],\n",
      "       [ 0.14669418]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = p1.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcc895f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 2000 rounds, with 41 training batches and 50 inner rounds\n",
      "training batch 0\n",
      "Rounds 0\n",
      "length of buffer is 66\n",
      "length of buffer is 59\n",
      "training batch 1\n",
      "Rounds 0\n",
      "length of buffer is 68\n",
      "length of buffer is 64\n",
      "training batch 2\n",
      "Rounds 0\n",
      "length of buffer is 66\n",
      "length of buffer is 55\n",
      "training batch 3\n",
      "Rounds 0\n",
      "length of buffer is 58\n",
      "length of buffer is 51\n",
      "training batch 4\n",
      "Rounds 0\n",
      "length of buffer is 60\n",
      "length of buffer is 54\n",
      "training batch 5\n",
      "Rounds 0\n",
      "length of buffer is 56\n",
      "length of buffer is 55\n",
      "training batch 6\n",
      "Rounds 0\n",
      "length of buffer is 54\n",
      "length of buffer is 55\n",
      "training batch 7\n",
      "Rounds 0\n",
      "length of buffer is 65\n",
      "length of buffer is 68\n",
      "training batch 8\n",
      "Rounds 0\n",
      "length of buffer is 56\n",
      "length of buffer is 53\n",
      "training batch 9\n",
      "Rounds 0\n",
      "length of buffer is 50\n",
      "length of buffer is 45\n",
      "training batch 10\n",
      "Rounds 0\n",
      "length of buffer is 67\n",
      "length of buffer is 66\n",
      "training batch 11\n",
      "Rounds 0\n",
      "length of buffer is 53\n",
      "length of buffer is 48\n",
      "training batch 12\n",
      "Rounds 0\n",
      "length of buffer is 56\n",
      "length of buffer is 51\n",
      "training batch 13\n",
      "Rounds 0\n",
      "length of buffer is 67\n",
      "length of buffer is 58\n",
      "training batch 14\n",
      "Rounds 0\n",
      "length of buffer is 63\n",
      "length of buffer is 58\n",
      "training batch 15\n",
      "Rounds 0\n",
      "length of buffer is 56\n",
      "length of buffer is 49\n",
      "training batch 16\n",
      "Rounds 0\n",
      "length of buffer is 63\n",
      "length of buffer is 52\n",
      "training batch 17\n",
      "Rounds 0\n",
      "length of buffer is 58\n",
      "length of buffer is 49\n",
      "training batch 18\n",
      "Rounds 0\n",
      "length of buffer is 56\n",
      "length of buffer is 50\n",
      "training batch 19\n",
      "Rounds 0\n",
      "length of buffer is 63\n",
      "length of buffer is 57\n",
      "training batch 20\n",
      "Rounds 0\n",
      "length of buffer is 71\n",
      "length of buffer is 62\n",
      "training batch 21\n",
      "Rounds 0\n",
      "length of buffer is 69\n",
      "length of buffer is 63\n",
      "training batch 22\n",
      "Rounds 0\n",
      "length of buffer is 59\n",
      "length of buffer is 49\n",
      "training batch 23\n",
      "Rounds 0\n",
      "length of buffer is 65\n",
      "length of buffer is 55\n",
      "training batch 24\n",
      "Rounds 0\n",
      "length of buffer is 68\n",
      "length of buffer is 58\n",
      "training batch 25\n",
      "Rounds 0\n",
      "length of buffer is 68\n",
      "length of buffer is 62\n",
      "training batch 26\n",
      "Rounds 0\n",
      "length of buffer is 48\n",
      "length of buffer is 51\n",
      "training batch 27\n",
      "Rounds 0\n",
      "length of buffer is 61\n",
      "length of buffer is 61\n",
      "training batch 28\n",
      "Rounds 0\n",
      "length of buffer is 70\n",
      "length of buffer is 65\n",
      "training batch 29\n",
      "Rounds 0\n",
      "length of buffer is 61\n",
      "length of buffer is 57\n",
      "training batch 30\n",
      "Rounds 0\n",
      "length of buffer is 72\n",
      "length of buffer is 66\n",
      "training batch 31\n",
      "Rounds 0\n",
      "length of buffer is 51\n",
      "length of buffer is 53\n",
      "training batch 32\n",
      "Rounds 0\n",
      "length of buffer is 58\n",
      "length of buffer is 55\n",
      "training batch 33\n",
      "Rounds 0\n",
      "length of buffer is 48\n",
      "length of buffer is 42\n",
      "training batch 34\n",
      "Rounds 0\n",
      "length of buffer is 65\n",
      "length of buffer is 65\n",
      "training batch 35\n",
      "Rounds 0\n",
      "length of buffer is 50\n",
      "length of buffer is 46\n",
      "training batch 36\n",
      "Rounds 0\n",
      "length of buffer is 70\n",
      "length of buffer is 64\n",
      "training batch 37\n",
      "Rounds 0\n",
      "length of buffer is 69\n",
      "length of buffer is 60\n",
      "training batch 38\n",
      "Rounds 0\n",
      "length of buffer is 62\n",
      "length of buffer is 55\n",
      "training batch 39\n",
      "Rounds 0\n",
      "length of buffer is 69\n",
      "length of buffer is 67\n",
      "training batch 40\n",
      "Rounds 0\n",
      "length of buffer is 72\n",
      "length of buffer is 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = State(p1, p2)\n",
    "st.NNPlay(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c9aeacc-8018-411c-a566-548bd0a64390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.19710356, -0.44819626, -0.28192848, -0.03402208],\n",
      "       [ 0.08384394,  0.26053673, -0.14118801,  0.373423  ],\n",
      "       [ 0.1395128 ,  0.4672206 , -0.579222  , -0.11965451],\n",
      "       [-0.02389818,  0.3548066 ,  0.5967817 ,  0.24781917],\n",
      "       [ 0.2193257 ,  0.6259165 , -0.6420938 ,  0.6109241 ],\n",
      "       [-0.05803855, -0.13624585, -0.02749629, -0.4687885 ],\n",
      "       [-0.20110391, -0.60669804,  0.56789   , -0.56202483],\n",
      "       [ 0.11005239,  0.43870124, -0.3401159 , -0.47298747],\n",
      "       [-0.14242919,  0.14559047, -0.44263825,  0.4016926 ]],\n",
      "      dtype=float32), array([ 0.00393651, -0.00859887,  0.00911706, -0.00299063], dtype=float32)]\n",
      "{'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.65565854],\n",
      "       [-0.1110415 ],\n",
      "       [ 0.06716845],\n",
      "       [-0.00173528]], dtype=float32), array([0.02949391], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = st.p1.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f5e622-c8cc-49e0-8a63-ecb5ddd72466",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.p1.state_value_model.save('p1model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b5fdf71-a8d4-40ce-be80-04e027009444",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.p2.state_value_model.save('p2model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4efd49-35e2-451f-895f-a286035ff49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dense_2', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 4, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[-0.4538742 , -0.17369892,  0.17416154, -0.34442285],\n",
      "       [ 0.31063217, -0.48849082,  0.21157224,  0.62748605],\n",
      "       [ 0.5303052 , -0.03440766, -0.44713694,  0.62045246],\n",
      "       [-0.27354816,  0.11483011, -0.49350196, -0.4947174 ],\n",
      "       [-0.22272362,  0.6493644 , -0.4952509 , -0.5718568 ],\n",
      "       [-0.5238724 , -0.44496045,  0.09623165, -0.32418066],\n",
      "       [ 0.5510279 , -0.5399643 ,  0.02654815,  0.13130866],\n",
      "       [ 0.67515653, -0.28145   , -0.47561413,  0.30561885],\n",
      "       [ 0.09916825,  0.17728382, -0.38563403, -0.42024794]],\n",
      "      dtype=float32), array([ 0.00236381, -0.01300867,  0.06608664,  0.02272303], dtype=float32)]\n",
      "{'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None} [array([[ 0.06556919],\n",
      "       [-0.23981471],\n",
      "       [-0.16691025],\n",
      "       [-0.09169208]], dtype=float32), array([-0.06462637], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "mm = st.p2.state_value_model\n",
    "for layer in mm.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dafc17-a8d7-4ba1-988d-2d0be111ed1a",
   "metadata": {},
   "source": [
    "# Check learning by playing against opponent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea873566-dd64-488e-b8b4-72a6e6d0171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = Player(\"p3\", eps_decay = False, start_exp_rate=0, end_exp_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "000c87b0-5c5b-416f-9362-b88b00701889",
   "metadata": {},
   "outputs": [],
   "source": [
    "p3.loadSVNNmodel('p1model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4197722-cdb0-4406-b050-af77fab00153",
   "metadata": {},
   "outputs": [],
   "source": [
    "p4 = Player(\"p4\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844b7fb5-84ee-4808-a831-c6f4987057e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = HumanPlayer('Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52c145d7-c029-4da2-8eae-0f2d5e178cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = State(p3, p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1403677e-9e47-4669-85fd-1fe514436325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds 0\n"
     ]
    }
   ],
   "source": [
    "output = st.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e09e24-91c2-4f2d-9b87-3e2312d08ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37fa8b9a-2a1f-4167-89f7-048494953c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 7 4\n"
     ]
    }
   ],
   "source": [
    "print (output.count(1), output.count(-1), output.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e987afa-fb83-40db-a715-72db0eb59169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check learning by playing two random players against each other\n",
    "p5 = Player(\"p5\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)\n",
    "p6 = Player(\"p6\", eps_decay = False, start_exp_rate=1, end_exp_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ae1e413-51fb-44e1-9cac-6c5623617920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounds 0\n",
      "57 29 0\n"
     ]
    }
   ],
   "source": [
    "st = State(p5, p6)\n",
    "output = st.play(100)\n",
    "print (output.count(1), output.count(-1), output.count(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f145a5-1178-4a8b-b68e-3b9f8c018a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
