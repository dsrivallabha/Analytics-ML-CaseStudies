{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faad743f-96c0-4a9f-abdd-9ebbd6f797b4",
   "metadata": {},
   "source": [
    "# Simulating Grid World using Tensorflow \n",
    "### Ref: https://towardsdatascience.com/tf-agents-tutorial-a63399218309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3739a5-c33b-425b-b80d-a0980376da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment libraries\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.specs import array_spec\n",
    "import numpy as np\n",
    "\n",
    "# Simulation libraries\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.environments import wrappers\n",
    "\n",
    "#from tf_agents.metrics import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2dd0dc",
   "metadata": {},
   "source": [
    "# Setting up the Grid World Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the starting and ending point of the training\n",
    "global sx, sy, dx, dy\n",
    "sx, sy, dx, dy = 5, 5, 3, 3\n",
    "sxold, syold = 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b03e84-7991-4fdb-bd69-41f41c0729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(py_environment.PyEnvironment):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(4,), dtype=np.int32, minimum=[0,0,0,0],maximum=[5,5,5,5], name='observation')\n",
    "        \n",
    "        self._state=[sx, sy, dx, dy] #represent the (row, col, frow, fcol) of the player and the finish\n",
    "        self._steps = 0\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._state= [sx, sy, dx, dy]\n",
    "        self._steps = 0\n",
    "        self._episode_ended = False\n",
    "        return ts.restart(np.array(self._state, dtype=np.int32))\n",
    "\n",
    "    def _step(self, action):\n",
    "        \n",
    "        self._steps += 1\n",
    "\n",
    "        if (self._steps > 90):\n",
    "            self._episode_ended = True\n",
    "                \n",
    "        if self._episode_ended:\n",
    "            return self.reset()\n",
    "\n",
    "        # make a move\n",
    "        self.move(action)\n",
    "                \n",
    "        if self.game_over():\n",
    "            self._episode_ended = True\n",
    "\n",
    "        if self._episode_ended:\n",
    "            if self.game_over():\n",
    "                reward = 100\n",
    "            else:\n",
    "                reward = 0\n",
    "            return ts.termination(np.array(self._state, dtype=np.int32), reward)\n",
    "        else:\n",
    "            # give a negative reward in general\n",
    "            return ts.transition(\n",
    "                np.array(self._state, dtype=np.int32), reward=-0.1 * self.currdist(), discount=0.9)\n",
    "\n",
    "    def move(self, action):\n",
    "        row, col, frow, fcol = self._state[0],self._state[1],self._state[2],self._state[3]\n",
    "        if action == 0: #down\n",
    "            if row - 1 >= 0:\n",
    "                self._state[0] -= 1\n",
    "        if action == 1: #up\n",
    "            if row + 1 < 6:\n",
    "                self._state[0] += 1\n",
    "        if action == 2: #left\n",
    "            if col - 1 >= 0:\n",
    "                self._state[1] -= 1\n",
    "        if action == 3: #right\n",
    "            if col + 1  < 6:\n",
    "                self._state[1] += 1\n",
    "\n",
    "    def game_over(self):\n",
    "        row, col, frow, fcol = self._state[0],self._state[1],self._state[2],self._state[3]\n",
    "        return row==frow and col==fcol\n",
    "    \n",
    "    def currdist(self):\n",
    "        row, col, frow, fcol = self._state[0],self._state[1],self._state[2],self._state[3]\n",
    "        return (abs(row-frow) + abs(col-fcol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1096993-e73f-4a6f-a80f-e7c27432d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()\n",
    "utils.validate_py_environment(env, episodes=5)\n",
    "print ('Grid world is validated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7846212",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f3616",
   "metadata": {},
   "source": [
    "# Declaring the environment for the first time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a211283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Writing a function to declare the environment\n",
    "def declareenv(ox=sx, oy=sy):\n",
    "        \n",
    "    global sx, sy\n",
    "    sxold, syold = sx, sy\n",
    "    sx, sy = ox, oy \n",
    "    print ('new sx and sy are', sx, sy)\n",
    "    \n",
    "    train_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\n",
    "    train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "    print (train_env.reset())\n",
    "    \n",
    "    # reset back\n",
    "    #sx, sy = sxold, syold\n",
    "    \n",
    "    return (train_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a3872",
   "metadata": {},
   "source": [
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        train_env = declareenv(i,j)\n",
    "        print (i, j )\n",
    "        print (train_env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = declareenv()\n",
    "eval_env = declareenv()\n",
    "\n",
    "# Evaluate a few things\n",
    "print (train_env.action_spec())\n",
    "print (train_env.observation_spec())\n",
    "print (train_env.time_step_spec())\n",
    "print (train_env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a984f",
   "metadata": {},
   "source": [
    "# Set up the simulation run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d63ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average return \n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            total_return += episode_return\n",
    "\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bde13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "num_iterations = 10000  # @param\n",
    "\n",
    "initial_collect_steps = 1000  # @param\n",
    "collect_steps_per_iteration = 1  # @param\n",
    "replay_buffer_capacity = 10000  # @param\n",
    "\n",
    "fc_layer_params = (50,25)\n",
    "\n",
    "batch_size = 128  # @param\n",
    "learning_rate = 1e-5  # @param\n",
    "log_interval = 200  # @param\n",
    "\n",
    "num_eval_episodes = 2  # @param\n",
    "eval_interval = 1000  # @param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d445e6",
   "metadata": {},
   "source": [
    "# Setting up the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Q network\n",
    "q_net = q_network.QNetwork(\n",
    "        train_env.observation_spec(),\n",
    "        train_env.action_spec(),\n",
    "        fc_layer_params=fc_layer_params)\n",
    "\n",
    "# Set up the optimizer\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer = tf.optimizers.SGD (learning_rate=0.001, name='SGD')\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "# Set up the agent\n",
    "tf_agent = dqn_agent.DqnAgent(\n",
    "        train_env.time_step_spec(),\n",
    "        train_env.action_spec(),\n",
    "        q_network=q_net,\n",
    "        optimizer=optimizer,\n",
    "        td_errors_loss_fn = common.element_wise_squared_loss,  #dqn_agent.element_wise_squared_loss,\n",
    "        train_step_counter=train_step_counter)\n",
    "\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2264e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the policy\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "        data_spec=tf_agent.collect_data_spec,\n",
    "        batch_size=train_env.batch_size,\n",
    "        max_length=replay_buffer_capacity)\n",
    "\n",
    "print(\"Batch Size: {}\".format(train_env.batch_size))\n",
    "\n",
    "# Create reply observer\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "# Train metrics\n",
    "train_metrics = [\n",
    "            tf_metrics.NumberOfEpisodes(),\n",
    "            tf_metrics.EnvironmentSteps(),\n",
    "            tf_metrics.AverageReturnMetric(),\n",
    "            tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ab35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect trajectories\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "    \n",
    "# Set up the datastep and driver\n",
    "dataset = replay_buffer.as_dataset(\n",
    "            num_parallel_calls=3,\n",
    "            sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some prints\n",
    "episode_len = []\n",
    "step_len = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87bf530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc6238e",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ea1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.train = common.function(tf_agent.train)\n",
    "tf_agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d191664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onerun(env, episode_len, step_len):\n",
    "    \n",
    "    driver = dynamic_step_driver.DynamicStepDriver(\n",
    "            train_env,\n",
    "            collect_policy,\n",
    "            observers=replay_observer + train_metrics,\n",
    "    num_steps=1)\n",
    "\n",
    "    print(compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes))\n",
    "\n",
    "    final_time_step, policy_state = driver.run()\n",
    "\n",
    "    for i in range(100):\n",
    "        final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        final_time_step, _ = driver.run(final_time_step, policy_state)\n",
    "        #for _ in range(1):\n",
    "        #    collect_step(train_env, tf_agent.collect_policy)\n",
    "\n",
    "        experience, _ = next(iterator)\n",
    "        train_loss = tf_agent.train(experience=experience)\n",
    "        step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "            episode_len.append(train_metrics[3].result().numpy())\n",
    "            step_len.append(step)\n",
    "            print('Average episode length: {}'.format(train_metrics[3].result().numpy()))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "\n",
    "\n",
    "    return (episode_len, step_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa34a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training 1: \n",
    "tuples = [(0,0), (5, 5), (0, 5), (5, 0)]\n",
    "\n",
    "#training 2:\n",
    "#tuples = [(0,0)]\n",
    "\n",
    "for t in tuples:\n",
    "    i, j = t[0], t[1]\n",
    "    print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print (i, j)\n",
    "    train_env = declareenv(i, j)\n",
    "    print (train_env.reset())\n",
    "    print ('###############################')\n",
    "    episode_len, step_len = onerun(train_env, episode_len, step_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c676dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(step_len, episode_len)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Episode Length (Steps)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b295a5a",
   "metadata": {},
   "source": [
    "# Evaluate if learnt policy is useful, by starting from different states\n",
    "\n",
    "##### Workaround to give a new starting point\n",
    "\n",
    "1. Four global variables, sx, sy, dx, dy are defined at the start of the simulation. \n",
    "2. Based on the required starting point these global sx and sy are modifed and environment is created\n",
    "3. After the simulation run, sx ,sy are reset to old state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b7516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newstartpoint(startpos):\n",
    "    '''\n",
    "    function to simulate starting from a required location\n",
    "    '''\n",
    "    global sx, sy\n",
    "    sxold, syold = sx, sy\n",
    "    # changing the global variables sx, sy that are used to declare environment\n",
    "    print ('previous values of global variables sx, sy are', sx, sy)\n",
    "    \n",
    "    sx = startpos[0]\n",
    "    sy = startpos[1]\n",
    "    \n",
    "    print ('new values of global variables sx, sy are', sx, sy)\n",
    "    \n",
    "\n",
    "    # declare a new tf env starting with Py env \n",
    "    check_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\n",
    "    check_env = tf_py_environment.TFPyEnvironment(check_py_env)\n",
    "    \n",
    "    # reset new environment\n",
    "    newenv = check_env\n",
    "    time_step = newenv.reset()\n",
    "    #time_step = newenv.set_state(startpos[0], startpos[1])\n",
    "    print (time_step)\n",
    "    \n",
    "    nsteps = 0\n",
    "    #run in loop\n",
    "    while not time_step.is_last():\n",
    "        action = tf_agent.policy.action(time_step)\n",
    "        print ('action take is', action.action)\n",
    "        time_step = newenv.step(action.action)\n",
    "        nsteps +=1\n",
    "        print ('new time step is', time_step)\n",
    "    \n",
    "    print ('time steps to reach destination from starpos is', nsteps)\n",
    "    \n",
    "    \n",
    "    # resetting sx and sy\n",
    "        # changing the global variables sx, sy that are used to declare environment\n",
    "    print ('current values of global variables sx, sy are', sx, sy)\n",
    "    \n",
    "    sx, sy = sxold, syold \n",
    "    \n",
    "    print ('updated values of global variables sx, sy are', sx, sy)\n",
    "    \n",
    "    return nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "marklst = []\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        nsteps = newstartpoint([i,j])\n",
    "        marklst.append([i, j, nsteps])\n",
    "        \n",
    "print (marklst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "marklst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb2a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c70ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
